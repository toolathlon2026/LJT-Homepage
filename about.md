# About John Doe

## Academic Background
PhD in Computer Science, Stanford University (2020)
MS in Artificial Intelligence, MIT (2017)
BS in Computer Engineering, MIT (2015)

## Research Experience
- Principal Research Scientist at OpenAI (2020-Present)
- Research Intern at DeepMind (2019)
- NLP Researcher at CMU (2017-2018)

## Publications
1. "Transformer Architectures for Multilingual NLP" (NeurIPS 2023)
2. "Efficient Vision Transformers" (ICML 2022)
3. "Continual Learning in Large Language Models" (ICLR 2021)

## Skills
- Machine Learning: PyTorch, TensorFlow, JAX
- NLP: BERT, GPT, T5
- Systems: Kubernetes, Docker, AWS
- Programming: Python, C++, Rust

## Contact
ðŸ“§ john.doe@example.com
ðŸ”— [LinkedIn](https://linkedin.com/in/johndoe)
ðŸ”— [GitHub](https://github.com/johndoe)